{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "\n",
    "Andrew O'Gorman - June 29, 2018\n",
    "\n",
    "## Proposal\n",
    "\n",
    "### Domain Background\n",
    "\n",
    "For my Capstone project I will be using a deep learning approach to attempt to solve the Humpback Whale Identification project on Kaggle <sup>[1]</sup>. This is a similar image classification problem to the dog breed classification challenge in the Deep Learning section of the Machine Learning Nanodegree. I have chosen this domain as I care deeply about our oceans and maritime life. I have been fascinated by whales since visiting the Natural History Museum<sup>[2]</sup>, London, as a child with my father and seeing the full skeleton of the blue whale. They are truly magnificent creatures; giants of the ocean and I feel passionately about helping organisations that support and monitor them. This project appeals to me as the work will help contribute to Happy Whale's <sup>[3]</sup> understanding of the movement of whales by using machine learning to dramatically increase the efficiency of this identification work. \n",
    "\n",
    "Whale tails (referred to as 'Flukes') are like a barcode or fingerprint, with enough information to identify an individual whale. Traditionally, scientists and marine biologists have taken and amassed large numbers of photograpahs of whales and then had to manually attempt to match newly photographed whales with historic images<sup>[4]</sup>. This process is time consuming and prone to a high degree of error, also there are challenges around getting the pictures due to the geographical spread of whales and the amount of time spent underwater. These are some of the reasons why this problem suits a machine learning approach.\n",
    "\n",
    "Previous work was done at the University of Texas<sup>[5]</sup> in 2003 to identify Humpback and Gray Whales using a patch-matching technique as a follow-up phase to WhaleNet once they specified the fluke type. More recently a team at the University of Catalunya in Barcelona<sup>[6]</sup> used convolutional neural networks to test the feasibility of using deep learning in whale recognition using the NOAA Fisheries dataset. Their paper outlines a successful approach to applying CNNs to identification of the heads of whales and so this seems like a good approach to build upon in this capstone project.\n",
    "\n",
    "I am keen to test my understanding of image recognition using Deep Learning as there are several additional projects I would like to conduct upon completion of my Nanodegree and so hope this Capstone Project will be the foundation of further work for me in this field.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The problem is to use the existing dataset of whale fluke images to build an understanding of each whale's unique characteristics of their tails. By using this understanding, we should then be able to take a new picture of a whale fluke and determine whether it matches a previously seen whale or whether it is in fact a new whale not previously seen in our dataset. \n",
    "\n",
    "This problem is an image recognition challenge given the unique features of a whale's fluke as seen below:\n",
    "\n",
    "\n",
    "<img src=\"files/fluke.jpg\">\n",
    "\n",
    "\n",
    "This problem is a good one to solve as understanding and tracking whale populations across the globe will help in several fields including ocean conservation and global climate change. \n",
    "\n",
    "\n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    "The dataset I will be using consists of over 25,000 images of whale flukes, with 9851 labelled images in the training set and 15,611 images in the testing set. In total the dataset contains images for 4250 different whales. Each image varies in size (number of pixels), colour, quality (sharpness) and orientation. \n",
    "\n",
    "The data was provided by Happy Whale, a citizen science organisation helping to track individual whales throughout the world's oceans. The images were gathered from research institutions and public contributions. The images specifically targeted whale flukes with the aim of being used to help identify the migration patterns of whales over time so as a dataset is ideally suited to the proposed problem.\n",
    "\n",
    "I will first look to pre-process the data to standardise the size, colour and proportions of all the images. This will allow me to build, train and test a deep learning algorithm to help identify whales within the dataset. The data is already split into training and testing sets, however, I will look to further subdivide the training set as I build my model.\n",
    "\n",
    "\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "\n",
    "I will be using a Deep Learning approach for this image identification problem. Deep Learning makes use of neural networks which aim to mimic how the brain operates with neurons used to fire pieces of information through a network to produce an output. By using this approach and building complex neural networks, Deep Learning can be more effective at determining the important features in a given image than a human. \n",
    "\n",
    "Specifically, I intend to use Convolutional Neural Networks (CNNs) for this problem. I have chosen to use CNNs for several reasons:\n",
    "\n",
    "-  CNNs maintain spatial information by taking matrices as inputs when compared to traditional Multilayer Perceptrons. This allows us to use fewer weights as some parameters are shared, hence lowering the computational cost and training time.\n",
    "\n",
    "- CNNs work well across images where there are distortions due to lighting conditions, horizontal/vertical shifts, different poses etc.\n",
    "\n",
    "- They are very good at identifying patterns within images by using filters to find specific groups of pixel groupings that are important.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "The benchmark score I will be comparing against was achieved using a technique known as Perceptual hashing (pHash)<sup>[8]</sup>. The pHash algorithm produces a fingerprint for each image which are analogous if features in the images are similar. This technique has been used previously to identify cases of online copyright infringement and also in digital forensics work due to its ability to have a correlation between hashes so similar images can be identified. \n",
    "\n",
    "The pHash technique is able to identify which whale IDs the image is most similar to and then submit their 5 most likely matches for each image. This benchmark submission was then measured using the MAP formula below to get a Mean Average Position score of 0.36075.\n",
    "\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The owners of the Kaggle competition hold a labelled list of the 15,611 testing images which result submissions are compared against. For each image in the test set I will predict up to 5 labels for the whale ID (e.g. **w_1287fbc**), where a whale is not predicted to be one of the existing whales in the training data they will be labelled as **new_whale**. The submissions file will contain a header and have the following format:\n",
    "\n",
    "    Image,Id\n",
    "    \n",
    "    00029b3a.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46\n",
    "    \n",
    "    0003c693.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46\n",
    "\n",
    "The submissions are evaluated according to the Mean Average Precision (MAP) as seen below:\n",
    "\n",
    "    MAP@5 = \\frac{1}{U} \\sum_{u=1}^{U}  \\sum_{k=1}^{min(n,5)} P(k)\n",
    "\n",
    "Where **U** is the number of images, **P(k)** is the precision at cut-off **k**, and **n** is the number predictions per image. Scores are between 0 and 1 with a score of 1 being a perfect match with no error.\n",
    "\n",
    "### Project Design\n",
    "\n",
    "\n",
    "#### Data Exploration\n",
    "I will explore the data in detail to build my understanding of its structure and any trends that exist within it. This will include looking at how many images there are per whaleID, how the images vary in characteristics such as image size and also how many if the images are in colour vs grayscale. \n",
    "\n",
    "#### Data Pre-processing\n",
    "I will look to run pre-processing of the training data in order to standardise the images. This will include converting the images to a set size as an initial look at the images suggests that the size of each image varies significantly. I noticed upon initial inspection that some of the images contain text characters in the margins (as seen in the image below) so I will look to remove this as it could skew the results. \n",
    "\n",
    "<img src=\"files/train/3ccc2a19.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "If there is variation between colour and grayscale images then I will likely convert all to grayscale. I will test to see whether I can crop the images around the flukes to reduce the background noise of the image (see below), and if so what impact this has on the accuracy of the model. \n",
    "\n",
    "<img src=\"files/train/6ca26bb1.jpg\" width=\"400\">\n",
    "\n",
    "Once happy with this data pre-processing I will run the exact same pre-processing pipeline for the testing dataset.\n",
    "\n",
    "#### Data Augmentation\n",
    "Looking at the total number of images in the training dataset (9851) and the number of different whale (4250) it suggests that there will be two or less images per whale for the CNN to learn from. This is very low for training a model on and so I will look to boost this using data augmentation to include image rotation, image shift, image zoom and image grayscaling. This will also help reduce the chance of overfitting.\n",
    "\n",
    "\n",
    "####  Model Design\n",
    "As previously stated I intend to use a Convolutional Neural Network for this problem. Given the size of the dataset I will be using keras for the pre-processing and training of the CNN. Similar to the dog breed classification project the CNN will be made up of an input layer with several convolutional layers separated by pooling layers which will deepen the spatial information while decreasing the spatial dimension of each image. I will include dropout layers and batch normalisation layers to counter any possible overfitting. The output will be passed through several fully connected Dense layers with 4551 nodes, one for each whaleID (including one for new whales). Finally the output will go through a softmax activation layer to produce a probability that the image matches each of the possible whales. \n",
    "\n",
    "The algorithm will be designed to provide the top 5 most likely whaleIDs as the prediction for each image. I will investigate whether a confidence threshold can be set to provide less than 5 answers when the likelihood of additional images are too low. I will need to tune the parameters for the layers within the CNN to optimise the model.\n",
    "\n",
    "\n",
    "#### Training and Testing the Model\n",
    "I will then compile and train the model using the augmented training dataset. This will include batching the training dataset into smaller subsections and training over multiple epochs, saving the best weights from the highest scoring epoch.\n",
    "\n",
    "I will then use these weights in my model to generate a submission for the testing set and submit this result set to Kaggle for scoring. Once I have received this score I will look to adapt and evolve my model and approach depending on the score. There are several additional things I would like to look at including whether there are any duplicate images within the dataset and if so how these can be handled most effectively, and also how a transfer learning approach could be used to help improve the accuracy of the model.\n",
    "\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "* https://www.kaggle.com/c/whale-categorization-playground\n",
    "* http://www.nhm.ac.uk/discover/news/2017/july/museum-unveils-hope-the-blue-whale-skeleton.html\n",
    "* https://happywhale.com/home\n",
    "* https://www.nationalgeographic.com/adventure/adventure-blog/2016/05/04/whos-that-whale-your-photo-could-help-i-d-a-humpback/\n",
    "* https://link.springer.com/chapter/10.1007/3-540-45103-X_16\n",
    "* https://arxiv.org/pdf/1604.05605.pdf\n",
    "* http://www.alaskahumpbacks.org/matching.html\n",
    "* https://en.wikipedia.org/wiki/Perceptual_hashing\n",
    "\n",
    "[1]: https://www.kaggle.com/c/whale-categorization-playground\n",
    "[2]: http://www.nhm.ac.uk/discover/news/2017/july/museum-unveils-hope-the-blue-whale-skeleton.html\n",
    "[3]: https://happywhale.com/home\n",
    "[4]: https://www.nationalgeographic.com/adventure/adventure-blog/2016/05/04/whos-that-whale-your-photo-could-help-i-d-a-humpback/\n",
    "[5]: https://link.springer.com/chapter/10.1007/3-540-45103-X_16\n",
    "[6]: https://arxiv.org/pdf/1604.05605.pdf\n",
    "[7]: http://www.alaskahumpbacks.org/matching.html\n",
    "[8]: https://en.wikipedia.org/wiki/Perceptual_hashing\n",
    "\n",
    "-----------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
