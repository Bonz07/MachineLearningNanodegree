{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Andrew O'Gorman - August 11, 2018\n",
    "\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "For my Capstone project I will be using a deep learning approach to attempt to solve the Humpback Whale Identification project on [Kaggle](https://www.kaggle.com/c/whale-categorization-playground). This is a similar image classification problem to the dog breed classification challenge in the Deep Learning section of the Machine Learning Nanodegree. I have chosen this domain as I care deeply about our oceans and maritime life. I have been fascinated by whales since visiting the [Natural History Museum](http://www.nhm.ac.uk/discover/news/2017/july/museum-unveils-hope-the-blue-whale-skeleton.html), London, as a child with my father and seeing the full skeleton of the blue whale. They are truly magnificent creatures; giants of the ocean and I feel passionately about helping organisations that support and monitor them. This project appeals to me as the work will help contribute to [Happy Whale's](https://happywhale.com/home) understanding of the movement of whales by using machine learning to dramatically increase the efficiency of this identification work. \n",
    "\n",
    "Whale tails (referred to as 'Flukes') are like a barcode or fingerprint, with enough information to identify an individual whale. [Traditionally](https://www.nationalgeographic.com/adventure/adventure-blog/2016/05/04/whos-that-whale-your-photo-could-help-i-d-a-humpback/), scientists and marine biologists have taken and amassed large numbers of photograpahs of whales and then had to manually attempt to match newly photographed whales with historic images. This process is time consuming and prone to a high degree of error, also there are challenges around getting the pictures due to the geographical spread of whales and the amount of time spent underwater. These are some of the reasons why this problem suits a machine learning approach.\n",
    "\n",
    "Previous work was done at the [University of Texas](https://link.springer.com/chapter/10.1007/3-540-45103-X_16) in 2003 to identify Humpback and Gray Whales using a patch-matching technique as a follow-up phase to WhaleNet once they specified the fluke type. More recently a team at the [University of Catalunya in Barcelona](https://arxiv.org/pdf/1604.05605.pdf) used convolutional neural networks to test the feasibility of using deep learning in whale recognition using the NOAA Fisheries dataset. Their paper outlines a successful approach to applying CNNs to identification of the heads of whales and so this seems like a good approach to build upon in this capstone project.\n",
    "\n",
    "I am keen to test my understanding of image recognition using Deep Learning as there are several additional projects I would like to conduct upon completion of my Nanodegree and so hope this Capstone Project will be the foundation of further work for me in this field.\n",
    "\n",
    "The dataset I will be using to train the model can be found [here](https://www.kaggle.com/c/whale-categorization-playground/download/train.zip) and the testing dataset [here](https://www.kaggle.com/c/whale-categorization-playground/download/test.zip).\n",
    "\n",
    "### Problem Statement\n",
    "The problem is to use the existing dataset of whale fluke images to build an understanding of each whale's unique characteristics of their tails. By using this understanding, we should then be able to take a new picture of a whale fluke and determine whether it matches a previously seen whale or whether it is in fact a new whale not previously seen in our dataset. \n",
    "\n",
    "This problem is an image recognition challenge given the unique features of a [whale's fluke](http://www.alaskahumpbacks.org/matching.html) as seen below:\n",
    "\n",
    "<img src=\"files/fluke.jpg\">\n",
    "\n",
    "My anticipated approach is to apply Deep Learning to this problem. I will first need to analyse the data for any features that will need some form of pre-processing, for example image size, and will make an assessment on whether data augmentation is required. My aim is to then build and train a Convolutional Neural Network to identify patterns within each set of labelled whale images. I will use the optimal weights from this trained CNN to predict which whale each of the 15,000 test images refers to.\n",
    "\n",
    "This problem is a good one to solve as understanding and tracking whale populations across the globe will help in several fields including ocean conservation and global climate change. \n",
    "\n",
    "### Metrics\n",
    "The owners of the Kaggle competition hold a labelled list of the 15,611 testing images which result submissions are compared against. For each image in the test set I will predict up to 5 labels for the whale ID (e.g. **w_1287fbc**), where a whale is not predicted to be one of the existing whales in the training data they will be labelled as **new_whale**. The submissions file will contain a header and have the following format:\n",
    "\n",
    "    Image,Id\n",
    "    \n",
    "    00029b3a.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46\n",
    "    \n",
    "    0003c693.jpg,new_whale w_1287fbc w_98baff9 w_7554f44 w_1eafe46\n",
    "\n",
    "The submissions are evaluated according to the Mean Average Precision (MAP) as seen below:\n",
    "\n",
    "    MAP@5 = \\frac{1}{U} \\sum_{u=1}^{U}  \\sum_{k=1}^{min(n,5)} P(k)\n",
    "\n",
    "Where **U** is the number of images, **P(k)** is the precision at cut-off **k**, and **n** is the number predictions per image. Scores are between 0 and 1 with a score of 1 being a perfect match with no error.\n",
    "\n",
    "MAP is used as a metric instead of simple accuracy scores for object detection problems as they can sometimes introduce biases and also because it is important to assess the risk of misclassifications. MAP essentially creates a confidence score for all of the predictions by taking the average precision score for each set of predictions for a test image. The average across all testing images is then calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "The dataset I am using was provided by Happy Whale, a citizen science organisation helping to track individual whales throughout the world's oceans. The images were gathered from research institutions and public contributions. The images specifically targeted whale flukes with the aim of being used to help identify the migration patterns of whales over time so as a dataset is ideally suited to the proposed problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "training_df = pd.read_csv('../capstone/train.csv')\n",
    "number_whales = len(training_df['Id'].unique())\n",
    "\n",
    "print(\"Total images in the training data =\",len(os.listdir(\"../capstone/train\")) )\n",
    "print(\"Total images in the test data = \",len(os.listdir(\"../capstone/test\")))\n",
    "print(\"Total unique whales in training data =\",number_whales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "The data consists of over 25,000 images of whale flukes, with 9850 labelled images in the training set and 15,610 images in the testing set. In total the dataset contains images for 4251 different whales if you include the category of new_whale. Each image varies in size (number of pixels), colour, quality (sharpness) and orientation as we can see below:\n",
    "\n",
    "| Colour landscape | Grayscale and blurry | Colour portrait |\n",
    "| - | - | - |\n",
    "|<img src=\"files/train/6c54a646.jpg\">|<img src=\"files/train/e976465c.jpg\">|<img src=\"files/train/ea5f45ca.jpg\">|\n",
    "\n",
    "I will first look to pre-process the data to standardise the size, colour and proportions of all the images. This will allow me to build, train and test a deep learning algorithm to help identify whales within the dataset. The data is already split into training and testing sets, however, I will look to further subdivide the training set as I build my model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "buckets = Counter(training_df['Id'].value_counts().values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(buckets)), list(buckets.values())[::-1])\n",
    "plt.xticks(range(len(buckets)), list(buckets.keys())[::-1])\n",
    "plt.title(\"Number of images per whale\")\n",
    "plt.xlabel('Number of images')\n",
    "plt.ylabel('Whales')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial analysis of the data reveals that of the 4251 whales in the dataset over 2000 of them have only one image, with around 4000 of the whales having less than 5 images to train on. I will therefore use Data Augmentation to help boost the number of training images for each whale which I will discuss below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def is_grey_scale(img_path):\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    w,h = im.size\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            r,g,b = im.getpixel((i,j))\n",
    "            if r != g != b: return False\n",
    "    return True\n",
    "\n",
    "greyscale = [is_grey_scale(f'../capstone/train/{i}') for i in training_df['Image'].sample(frac=0.1)]\n",
    "grey_percentage = round(sum([i for i in greyscale]) / len([i for i in greyscale]) * 100, 2)\n",
    "\n",
    "print(f\"Percentage of grey images: {grey_percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the above [code]((https://stackoverflow.com/questions/23660929/how-to-check-whether-a-jpeg-image-is-color-or-gray-scale-using-only-python-stdli) we can see that we have almost an equal split of grayscale and colour images in the training data set. I will therefore look to convert all images (both training and testing) to greyscale in my data pre-processing step.\n",
    "\n",
    "Given the three images above are all different sizes I can also explore how the size of the images varies across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_sizes = Counter([Image.open(f'../capstone/train/{i}').size for i in training_df['Image']])\n",
    "\n",
    "size, freq = zip(*Counter({i: v for i, v in img_sizes.items() if v > 1}).most_common(20))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.bar(range(len(freq)), list(freq), align='center')\n",
    "plt.xticks(range(len(size)), list(size), rotation=90)\n",
    "plt.title(\"Image size counts\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see the image size varies significantly across the dataset. I will look to address this in the data pre-processing section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "I will be using a Deep Learning approach for this image identification problem. Deep Learning makes use of neural networks which aim to mimic how the brain operates with neurons used to fire pieces of information through a network to produce an output. By using this approach and building complex neural networks, Deep Learning can be more effective at determining the important features in a given image than a human. \n",
    "\n",
    "Specifically, I intend to use Convolutional Neural Networks (CNNs) for this problem. I have chosen to use CNNs for several reasons:\n",
    "\n",
    "-  CNNs maintain spatial information by taking matrices as inputs when compared to traditional Multilayer Perceptrons. This allows us to use fewer weights as some parameters are shared, hence lowering the computational cost and training time.\n",
    "\n",
    "\n",
    "- CNNs work well across images where there are distortions due to lighting conditions, horizontal/vertical shifts, different poses etc.\n",
    "\n",
    "\n",
    "- They are very good at identifying patterns within images by using filters to find specific groups of pixel groupings that are important.\n",
    "\n",
    "At the core of the convolutional neural network is the **convolutional layer**. It works by generating a convolutional window that moves across the image selecting a subset of pixels which is combines into a single hidden node (convolved feature). For a convolutional layer we define the stride, which is the amount by which the filter slides over the image, and the size of the filters, size of the convolutional window. In the below example the stride is set to 1 and the filter size is (3x3):\n",
    "\n",
    "![convolutional layer](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/19273/1533845716/Convolution_schematic.gif)\n",
    "\n",
    "The filter weights used in the convolutional window are learnt by the network with the weights chosen that minimise the loss function. The CNN will have several filters per layer which each generate their own convolved feature, each being setup to detect different types of features in the image.\n",
    "\n",
    "**Pooling layers** are then used to take this output of convolutional layers and reduce the dimensionality of the model to avoid over fitting. Again, we define the stride and the size of the filters, for example:\n",
    "\n",
    "![pooling layer](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/19273/1533845720/maxpool__1_.jpeg)\n",
    "\n",
    "We then can layer multiple convolutional and pooling layers within the CNN that take the output from the previous convolutional layer and apply the same techniques to the simplified output to search for patterns within the patterns.\n",
    "\n",
    "### Benchmark\n",
    "The benchmark score I will be comparing against was achieved using a technique known as [Perceptual hashing](https://en.wikipedia.org/wiki/Perceptual_hashing) (pHash). The pHash algorithm produces a fingerprint for each image which are analogous if features in the images are similar. This technique has been used previously to identify cases of online copyright infringement and also in digital forensics work due to its ability to have a correlation between hashes so similar images can be identified. \n",
    "\n",
    "The pHash technique is able to identify which whale IDs the image is most similar to and then submit their 5 most likely matches for each image. This benchmark submission was then measured using the MAP formula below to get a Mean Average Position score of 0.36075."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "For this project I took several steps to pre-process and also augment the data. In the code below you will find the following steps:\n",
    "\n",
    "* import required Python libraries\n",
    "* loading of training and testing data\n",
    "* generation of pHash values to identify perfect matches\n",
    "* conversion of data into dataframes\n",
    "* conversion of all images to greyscale\n",
    "* conversion of all images to standard size\n",
    "\n",
    "Having researched the pHash technique I decided it would be useful for this project to generate the pHash for each image to help identify duplicate images. I was able to later use this pHash value to identify perfect matches of images in the training and testing dataset and eventually used this check in my final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Python, sklearn and keras libraries\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.fftpack\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define pHash functions and then generate pHash values for each of the training images\n",
    "\n",
    "def _binary_array_to_hex(arr):\n",
    "    h = 0\n",
    "    s = []\n",
    "    for i, v in enumerate(arr.flatten()):\n",
    "        if v: \n",
    "            h += 2**(i % 8)\n",
    "        if (i % 8) == 7:\n",
    "            s.append(hex(h)[2:].rjust(2, '0'))\n",
    "            h = 0\n",
    "    return \"\".join(s)\n",
    "\n",
    "class ImageHash(object):\n",
    "    def __init__(self, binary_array):\n",
    "        self.hash = binary_array\n",
    "    def __str__(self):\n",
    "        return _binary_array_to_hex(self.hash.flatten())\n",
    "    def __repr__(self):\n",
    "        return repr(self.hash)\n",
    "    def __sub__(self, other):\n",
    "        if other is None:\n",
    "            raise TypeError('Other hash must not be None.')\n",
    "        if self.hash.size != other.hash.size:\n",
    "            raise TypeError('ImageHashes must be of the same shape.', self.hash.shape, other.hash.shape)\n",
    "        return (self.hash.flatten() != other.hash.flatten()).sum()\n",
    "    def __eq__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return numpy.array_equal(self.hash.flatten(), other.hash.flatten())\n",
    "    def __ne__(self, other):\n",
    "        if other is None:\n",
    "            return False\n",
    "        return not numpy.array_equal(self.hash.flatten(), other.hash.flatten())\n",
    "    def __hash__(self):\n",
    "        # this returns a 8 bit integer, intentionally shortening the information\n",
    "        return sum([2**(i % 8) for i, v in enumerate(self.hash.flatten()) if v])\n",
    "\n",
    "def phash(image, hash_size=8, highfreq_factor=4):\n",
    "    img_size = hash_size * highfreq_factor\n",
    "    image = image.convert(\"L\").resize((img_size, img_size), Image.ANTIALIAS)\n",
    "    pixels = np.array(image.getdata(), dtype=np.float).reshape((img_size, img_size))\n",
    "    dct = scipy.fftpack.dct(scipy.fftpack.dct(pixels, axis=0), axis=1)\n",
    "    dctlowfreq = dct[:hash_size, :hash_size]\n",
    "    med = np.median(dctlowfreq)\n",
    "    diff = dctlowfreq > med\n",
    "    return ImageHash(diff)\n",
    "\n",
    "def getImageMetaData(file_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        img_hash = phash(img)\n",
    "        return img.size, img.mode, img_hash\n",
    "\n",
    "def get_train_input():\n",
    "    train_input = pd.read_csv(r\"../capstone/train.csv\")\n",
    "    m = train_input.Image.apply(lambda x: getImageMetaData(r\"../capstone/train/\" + x))\n",
    "    train_input[\"Hash\"] = [str(i[2]) for i in m]\n",
    "    train_input[\"Shape\"] = [i[0] for i in m]\n",
    "    train_input[\"Mode\"] = [str(i[1]) for i in m]\n",
    "    train_input[\"Length\"] = train_input[\"Shape\"].apply(lambda x: x[0]*x[1])\n",
    "    train_input[\"Ratio\"] = train_input[\"Shape\"].apply(lambda x: x[0]/x[1])\n",
    "    train_input[\"New_Whale\"] = train_input.Id == \"new_whale\"\n",
    "    img_counts = train_input.Id.value_counts().to_dict()\n",
    "    train_input[\"Id_Count\"] = train_input.Id.apply(lambda x: img_counts[x])\n",
    "    return train_input\n",
    "\n",
    "train_input = get_train_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As later discussed in the Refinement section I transformed the training dataset to remove the images that were labelled 'new_whale'. As was shown in the Exploratory Analysis section this category had 810 images, far outweighing any of the other label categories. As this category essentially contained unlabelled images from a variety of different whale's flukes it was causing issues with the model when attempting to identify patterns across this category. \n",
    "\n",
    "Given this was the largest category by some way I decided to remove it from the training data so my model could learn more effectively. As the Kaggle competition allowed for up to five guesses for the correct label I hardcoded one of the guesses in this case to be 'new_whale' to mitigate for removal of this category earlier in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data into dataframes\n",
    "\n",
    "train_images = glob(\"../capstone/train/*jpg\")\n",
    "test_images = glob(\"../capstone/test/*jpg\")\n",
    "df = pd.read_csv(\"../capstone/train.csv\")\n",
    "\n",
    "# remove new_whale labelled images from df and train_images\n",
    "\n",
    "df_others = df.loc[(df[\"Id\"]!=\"new_whale\")]\n",
    "df_others_list = df_others['Image'].tolist()\n",
    "\n",
    "new_list = []\n",
    "for i in df_others_list:\n",
    "    new_list.append(\"../capstone/train/\"+i)\n",
    "\n",
    "df = df_others\n",
    "train_images = new_list\n",
    "\n",
    "# generate Image to Label mapping\n",
    "\n",
    "df[\"Image\"] = df[\"Image\"].map( lambda x : \"../capstone/train/\"+x)\n",
    "ImageToLabelDict = dict( zip( df[\"Image\"], df[\"Id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my exploratory analysis showed, there was roughly an equal split of colour and greyscale images in the dataset. I there used the code below to convert all training images to greyscale. We also saw how the size of the images in the dataset also varied, the below code was used to convert all images to size 100 pixels x 100 pixels\n",
    "\n",
    "I would later use this function to also convert all the testing images to greyscale and 100x100 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resize images and convert to black and white\n",
    "\n",
    "SIZE = 100\n",
    "\n",
    "def ImportImage(filename):\n",
    "    img = Image.open(filename).convert(\"LA\").resize( (SIZE,SIZE))\n",
    "    return np.array(img)[:,:,0]\n",
    "\n",
    "train_img = np.array([ImportImage(img) for img in train_images])\n",
    "x = train_img\n",
    "\n",
    "# check to see if resizing and greyscale conversion have worked\n",
    "\n",
    "def plotImages( images_arr, n_images=4):\n",
    "    fig, axes = plt.subplots(n_images, n_images, figsize=(12,12))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        if img.ndim != 2:\n",
    "            img = img.reshape( (SIZE,SIZE))\n",
    "        ax.imshow( img, cmap=\"Greys_r\")\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.tight_layout()\n",
    "\n",
    "plotImages(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the displayed images, the greyscale conversion and resizing worked and so I was able to move onto the Data Augmentation stage. \n",
    "\n",
    "Before starting this I needed to transform the training data for use in the Convolutional Neural Net, in order to do this I used One Hot Encoding. This technique allowed me to assign each categorical label (e.g. whale label) and convert this into an integer and then map each of the training images to this new integer value ready for input into the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding of the images\n",
    "\n",
    "class LabelOneHotEncoder():\n",
    "    def __init__(self):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.le = LabelEncoder()\n",
    "    def fit_transform(self,x):\n",
    "        features = self.le.fit_transform(x)\n",
    "        return self.ohe.fit_transform(features.reshape(-1,1))\n",
    "    def transform(self, x):\n",
    "        return self.ohe.transform(self.la.transform(x.reshape(-1,1)))\n",
    "    def inverse_tranform(self, x):\n",
    "        return self.le.inverse_transform( self.ohe.inverse_tranform(x))\n",
    "    def inverse_labels(self, x):\n",
    "        return self.le.inverse_transform(x)\n",
    "\n",
    "y = list(map(ImageToLabelDict.get, train_images))\n",
    "lohe = LabelOneHotEncoder()\n",
    "y_cat = lohe.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "As was evident from the exploratory analysis, the number of images per whale label for most categories was very low and in many cases just one image. To help boost the training images I decided upon using some heavy data augmentation to build some of this image variation. I used keras ImageDataGenerator to rescale, rotate, shift, zoom and horizontally flip the images. I then visualised some of the augmented images to visually confirm this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data augmentation using keras ImageDataGenerator\n",
    "\n",
    "x = x.reshape( (-1,SIZE,SIZE,1))\n",
    "input_shape = x[0].shape\n",
    "x_train = x.astype(\"float32\")\n",
    "y_train = y_cat\n",
    "\n",
    "image_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "image_gen.fit(x_train, augment=True)\n",
    "\n",
    "#visualization of some images out of the preprocessing\n",
    "augmented_images, _ = next( image_gen.flow( x_train, y_train.toarray(), batch_size=4*4))\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "I chose to implement a Convolutional Neural Network for this image detection problem. The biggest challenge here was how to design this network to best fit this type of data problem. I started with what I thought was a fairly simple CNN that would run quickly to test my code with the plan of building in complexity over time.\n",
    "\n",
    "The CNN I initially built had three Convolutional Layers with the number of features increasing as you progressed through the layers. I used a ReLu activation function in the first layer and the Sigmoid activation functions for each of the next two layers and a kernal size of 3. Each convolutional layer was separated by a MaxPooling layer. At the end of the CNN I had a flatten layer to convert the image matrices into vectors and then several dropout layers (with probability 0.33) to minimise overfitting. Finally I had a dense layer with 4251 nodes, one for each whale type.\n",
    "\n",
    "For this CNN I decided to use categorical crossentropy loss function as I was attempting to work out which category each image best matched with. I used the Adadelta optimiser for the Gradient Descent challenge as it typically runs faster than the Adam Optimiser. Finally I included the accuracy metric which was what I was hoping to measure.\n",
    "\n",
    "The initial CNN worked but performed poorly. It struggled to learn and minimise the loss function and also each set of predictions was the same for all test images. I realised that I needed to improve the CNN architecture and also address some of the data inbalancing caused by the large number of training images under the 'new_whale' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "As explained in the previous section over the course of this project I made several changes to the CNN architecture before settling on the one used to generate my final predictions. There were also several key refinements made to my approach before arriving at my end result.\n",
    "\n",
    "Firstly after seeing the output of the initial CNN it was clear that I had a problem with data inbalancing as each prediction was the same regardless of the testing image used. I had tried to mitigate the fact that the 'new_whale' category had far more images than any other whale type by introducing weights for each class which was a function of the number of images in each class, however this wasn't having the desired effect. I instead chose to remove all images under the new_whale category from the training data so they wouldn't affect my model. This allowed me to also remove the class weight function I had been using. I mitigated this by recognising that given it was by far the most likely category I would build in part of the prediction function at the end to choose 'new_whale' as one of the five predictions unless it was an exact match.\n",
    "\n",
    "After learning more about the pHash function to understand the benchmark score better I decided I could usefully implement an aspect of this in my code to identify when there was a perfect match for an image. In these cases, where the pHash of the testing image matched a pHash of a training image, instead of choosing the 5 most likely images I would only predict the one matched image.\n",
    "\n",
    "I attempted to introduce both an upper and lower limit threshold for predictions, for example when the first of the top 5 predictions exceeded this threshold only predict this one image rather than the top five. Similarly, when the last of the 5 predictions was below the lower confidence limit remove this from the predictions. I started with an upper threshold of 75% confidence and a lower limit of 5% confidence, this had the effect of significantly lowering my final score. I tried several versions of this until eventually settling on just a lower limit of not including predictions of less than 1% confidence.\n",
    "\n",
    "Finally, once happy with my code I gradually increased the number of epochs the code ran over, initially starting at 10 then 15, 25, 100, 250 before finally running my model over 1000 epochs which took several hours to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "The final model chosen was one that performed most effectively for this image classification problem. \n",
    "\n",
    "After several attempts at refining, the CNN the architecture I settled upon had two Convolutional Layers separated by a Max Pooling Function. The convolutional layers both used strides = (1,1) and deployed a ReLu activation function. The features increased from 32 to 64 as it proceeded through the CNN to provide more depth to the classifications as it better understands the image.\n",
    "\n",
    "I used an AveragePooling layer at the end of the CNN to reduce the size of the preceding layer to (15,15,64) by taking the average of each feature map. The next Flatten layer merely flattens the input, without resulting in any change to the information contained in the previous layer. I used a fully connected Dense layer with 500 nodes and ReLu activation function and then included a Dropout layer (with 0.8 probability) to reduce any overfitting. Finally I passed the data through a dense layer at the end with 4250 nodes, one for each whale type (when you exclude the new_whale type).\n",
    "\n",
    "For this final CNN I again used categorical crossentropy loss function but this time used the Adam Optimiser instead of Adadelta. Again I included the accuracy metric to measure against.\n",
    "\n",
    "The previous CNN architecture didn't improve it's learning rate over time and the loss function plateaued. With this architecture I was able to develop a model that not only learnt over time but also managed to minimise the loss function significantly over time. \n",
    "\n",
    "The final CNN used can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build and train model\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = len(y_cat.toarray()[0])\n",
    "epochs = 1000\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (7, 7), strides = (1,1), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), strides = (1,1), activation='relu'))\n",
    "model.add(AveragePooling2D((3, 3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit_generator(image_gen.flow(x_train, y_train.toarray(), batch_size=batch_size),\n",
    "          steps_per_epoch=  x_train.shape[0]//batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final code for generating predictions for testing data and producing sample submission output file\n",
    "\n",
    "import warnings\n",
    "from os.path import split\n",
    "\n",
    "TEST_IMG_PATH = r\"../capstone/test\"\n",
    "\n",
    "test_list = []\n",
    "with open(\"sample_submission.csv\",\"w\") as f:\n",
    "    with warnings.catch_warnings():\n",
    "        f.write(\"Image,Id\\n\")\n",
    "        \n",
    "        warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "        for image in test_images:\n",
    "            img = Image.open(image)\n",
    "            img_hash = str(phash(img))\n",
    "            dataf = train_input[train_input['Hash'].astype(str).str.contains(img_hash)]\n",
    "\n",
    "            if len(dataf.index) != 0:          \n",
    "                image_row = train_input.loc[train_input['Hash'] == img_hash]\n",
    "                image_row_ID = image_row['Id']\n",
    "                predicted_tags = image_row_ID.iloc[0]\n",
    "                image = split(image)[-1]\n",
    "                f.write(\"%s,%s\\n\" %(image, predicted_tags))\n",
    "                \n",
    "            else:\n",
    "                img = ImportImage( image)\n",
    "                x = img.astype( \"float32\")\n",
    "                x = image_gen.standardize( x.reshape(1,SIZE,SIZE))\n",
    "                y = model.predict_proba(x.reshape(1,SIZE,SIZE,1))\n",
    "                top_probs = np.sort(y)[0][::-1][:4]\n",
    "                \n",
    "                if top_probs[3] < 0.01:\n",
    "                    predicted_args = np.argsort(y)[0][::-1][:3]\n",
    "                    predicted_tags = lohe.inverse_labels(predicted_args)\n",
    "                    image = split(image)[-1]\n",
    "                    predicted_tags = \" \".join(predicted_tags)\n",
    "                    predicted_tags = \"new_whale\" + \" \" + predicted_tags\n",
    "                    f.write(\"%s,%s\\n\" %(image, predicted_tags))\n",
    "                \n",
    "                else:\n",
    "                    predicted_args = np.argsort(y)[0][::-1][:4]\n",
    "                    predicted_tags = lohe.inverse_labels(predicted_args)\n",
    "                    image = split(image)[-1]\n",
    "                    predicted_tags = \" \".join(predicted_tags)\n",
    "                    predicted_tags = \"new_whale\" + \" \" + predicted_tags\n",
    "                    f.write(\"%s,%s\\n\" %(image, predicted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test to see if output has formatted correctly for submission file, sample first 10 rows\n",
    "\n",
    "import csv\n",
    "with open('sample_submission.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for i,row in enumerate(reader):\n",
    "        print(row)\n",
    "        if(i >= 9):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the robustness of this model I wanted to introduce some random gaussian noise into some of the images and test how well the prediction model performs before and after this noise is added. \n",
    "\n",
    "For this I chose three images:\n",
    "\n",
    "* 9c741147.jpg\n",
    "* 7bf4a206.jpg\n",
    "* 7ade6a53.jpg\n",
    "\n",
    "I used the following data augmentation code to add the noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#enable reverse lookup of images from predicted labels\n",
    "\n",
    "v = {}\n",
    "for key, value in sorted(ImageToLabelDict.items()):\n",
    "    v.setdefault(value, []).append(key)\n",
    "\n",
    "#image1 = '../capstone/test/9c741147.jpg'\n",
    "#probs1 = [0.84070462, 0.11267345]\n",
    "#labels1 = ['w_d3ef4b2' 'w_e700deb']\n",
    "\n",
    "#image2 = '../capstone/test/7bf4a206.jpg' \n",
    "#probs2= [0.67868924, 0.12464489]\n",
    "#labels2 = ['w_21e178f', 'w_98baff9']\n",
    "\n",
    "#image3 = '../capstone/test/7ade6a53.jpg' \n",
    "#probs3 = [ 0.45092112, 0.30773205]\n",
    "#labels3 = ['w_fce6ab2', 'w_c06dd6e']\n",
    "\n",
    "#img_arr = ['../capstone/test/9c741147.jpg', '../capstone/train/6af9c34b.jpg', '../capstone/train/69d946a0.jpg',\n",
    "#          '../capstone/test/7bf4a206.jpg' ,'../capstone/train/1ae99bfc.jpg','../capstone/train/06f3adae.jpg',\n",
    "#          '../capstone/test/7ade6a53.jpg','../capstone/train/3e093171.jpg','../capstone/train/13151a25.jpg']\n",
    "\n",
    "#print(v['w_3cfeb1a'])\n",
    "\n",
    "def plotImage( images_arr, n_images=3):\n",
    "    fig, axes = plt.subplots(n_images, n_images, figsize=(12,12))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        if img.ndim != 2:\n",
    "            img = img.reshape( (SIZE,SIZE))\n",
    "        ax.imshow( img, cmap=\"Greys_r\")\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.tight_layout()\n",
    "\n",
    "img_array=[]\n",
    "Images=['../capstone/test/9c741147.jpg', '../capstone/train/6af9c34b.jpg', '../capstone/train/69d946a0.jpg',\n",
    "          '../capstone/test/7bf4a206.jpg' ,'../capstone/train/1ae99bfc.jpg','../capstone/train/06f3adae.jpg',\n",
    "          '../capstone/test/7ade6a53.jpg','../capstone/train/3e093171.jpg','../capstone/train/13151a25.jpg']\n",
    "\n",
    "\n",
    "for i in Images:\n",
    "    img = ImportImage(i)\n",
    "    img_array.append(img)\n",
    "\n",
    "plotImage(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define gaussian noise function\n",
    "\n",
    "def noisy(image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.1\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy\n",
    "\n",
    "## take image, put it through normal image cleanup and augmentation, then add noise\n",
    "image_list = ['../capstone/test/9c741147.jpg','../capstone/test/7bf4a206.jpg','../capstone/test/7ade6a53.jpg']\n",
    "img_array = ['../capstone/test/9c741147.jpg','../capstone/train/d157e710.jpg','../capstone/train/087495e6.jpg',\n",
    "             '../capstone/test/7bf4a206.jpg','../capstone/train/9b7694f8.jpg','../capstone/train/13d286ea.jpg',\n",
    "             '../capstone/test/7ade6a53.jpg','../capstone/train/68d47887.jpg','../capstone/train/b36a0348.jpg']\n",
    "\n",
    "sp_images = []\n",
    "\n",
    "for i in sp_images:\n",
    "    img = ImportImage(i)\n",
    "    img_array.append(img)\n",
    "\n",
    "for image in img_array:\n",
    "    if image in image_list:\n",
    "        img = ImportImage(image)\n",
    "        x = img.astype(\"float32\")\n",
    "        x = image_gen.standardize( x.reshape(1,SIZE,SIZE))\n",
    "        sp_image = noisy(x)\n",
    "        sp_images.append(sp_image)\n",
    "    \n",
    "        y = model.predict_proba(sp_image.reshape(1,SIZE,SIZE,1))\n",
    "        top_probs = np.sort(y)[0][::-1][:4]\n",
    "        predicted_args = np.argsort(y)[0][::-1][:3]\n",
    "        predicted_tags = lohe.inverse_labels(predicted_args)\n",
    "\n",
    "\n",
    "    else:\n",
    "        img = ImportImage(image)\n",
    "        sp_images.append(img)\n",
    "    \n",
    "\n",
    "# visualisation of images with noise\n",
    "plotImage(sp_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|image|Original match|Original score| With noise match| With noise score|\n",
    "|-----|:------------:|:------------:|:---------------:|:---------------:|\n",
    "|9c741147.jpg|w_d3ef4b2|84%|w_ec02248|99%|\n",
    "|9c741147.jpg|w_e700deb|11%|w_ff70408|<0.1%|\n",
    "|7bf4a206.jpg|w_21e178f|68%|w_2ac83b0|97%|\n",
    "|7bf4a206.jpg|w_98baff9|12%|w_6c99c53|0.1%|\n",
    "|7ade6a53.jpg|w_fce6ab2|45%|w_403f92f|81%|\n",
    "|7ade6a53.jpg|w_c06dd6e|31%|w_3cfeb1a|12%|\n",
    "\n",
    "We can see that by applying some random noise to the testing images it significantly affects the predictions of this model with all the images predicted being different to the first set. I would need to spend more time on this model to build in ways to handle this which is beyond the scope of this project, however it is worth noting that the model's predictions will vary if the testing images are altered in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "\n",
    "Using the results generated from the code above and after submitting my submissions file to the Kaggle competition I was able to achieve a score of 0.44320. This score not only beat the benchmark of 0.36075 but would have achieved a position of 55 (at the time of writing) on the global leaderboard for this Kaggle competition.\n",
    "\n",
    "The output from my code, as seen in the sample above, is a csv file containing 15,610 rows, each with between one and five predictions for the matching whale label. \n",
    "\n",
    "The result shows that there is significant value to be added to this type of problem by applying machine learning and specifically Deep Learning using Convolutional Neural Networks. This approach, whilst not solving the problem within this project did demonstrate how CNNs can be used to improve upon traditional hash matching techniques.\n",
    "\n",
    "Below is a table detailing how the score of my submissions matched and then exceeded the pHash benchmark as I refined my approach:\n",
    "\n",
    "| Score |\tDescription |\n",
    "|-------|:--------------:|\n",
    "|0.44320 |\tBest score, 1000 epochs with lower threshold |\n",
    "|0.44160 |\t99% upper threshold, 1000 epochs |\n",
    "|0.43995  |\t95% upper threshold, 1000 epochs |\n",
    "|0.43230 |\tFinal model run over 250 epochs |\n",
    "|0.42864 | 90% threshold, 1000 epochs |\n",
    "|0.41462|\tIncluded pHash matching, 250 epochs |\n",
    "|0.36075|\tpHash Benchmark |\n",
    "|0.32772 |\tInitial CNN architecture, 250 epochs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "In this section I wanted to demonstrate how the final model was able to learn over time. \n",
    "\n",
    "The original CNN was only able to achieve around 8% accuracy score before plateauing over 250 epochs. With my final CNN model I was able to run over 1000 epochs improving the accuracy score from an initial 0.2% to just over 40% accuracy. \n",
    "\n",
    "The visualisation below shows how the model accuracy improved over time as the CNN learned the optimal weights for the model. The accuracy increased at a rate of 4-5% per 50 epochs up until around 300 epochs. After 300 epochs the accuracy rate increase began to slow, however even by 1000 epochs was still increasing at a rate of 1% per 50 epochs.\n",
    "\n",
    "It would be interesting to see at what point this model stops improving its accuracy rate, however as running over 1000 epochs took several hours this wasn't something I was able to test during this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot how the accuracy changes as the model was trained\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "To summarise my project, I chose to look at the whale recognition challenge on Kaggle. The aim of this challenge was to see whether I could build a model that could take images of whale's tails (known as flukes) and match them to other labelled whale flukes. This project was sponsored by a non-profit organisation called Happy Whale who look to help better understand whale migration patterns by tracking whales across the globe using images submitted by the public. \n",
    "\n",
    "For this challenge I chose to apply Deep Learning by using Convolutional Neural Networks given their success in other image recognition problems. To begin with I spent time exploring the training data, identifying variation in image size, a near even split of colour to greyscale images and that for many of the whale types there were only one or two images per category. Where this differed was for the 'new_whale' category which had over 800 images.\n",
    "\n",
    "To test the performance of my model I planned to compare against the pHash benchmark which is a common image hashing technique, this achieved a prediction score of 0.36. \n",
    "\n",
    "The first part of my code covered the data pre-processing part of the project. This included converting all images to a standard size and converting all to greyscale. This pre-processing also included removing the 'new_whale' category of images from the training data to prevent data inbalancing when training the CNN. The final part of the pre-processing focused on the image augmentation, using the keras inbuilt augmentation function to rescale, rotate, shift, zoom and horizontally flip the training images. I did this to boost the size of the training dataset and build in variation across the images for each whale category.\n",
    "\n",
    "I then built my CNN and tested on the training data over a low number of epochs. I spent several iterations altering the CNN architecture until I was happy with how it was learning and reducing the loss function. I then began to increase the number of epochs this final model ran over from 25 up to eventually 1000 epochs.\n",
    "\n",
    "Using the optimum weights for the CNN from the previous training I was then able to run the 15,000 testing images over this CNN to produce my final submission sample. This sample was then submitted on the Kaggle competition website to generate my final score of 0.44, beating the benchmark.\n",
    "\n",
    "The final model achieves a top 55 score on the Kaggle competition of over 500 competitors so performed relatively well. There is definitely room for improvement and with more time I would have liked to use additional data pre-processing to isolate the whale flukes in the image to improve the focus of the CNN. Also, it would be interesting to use transfer learning to take pre-built CNNs that have performed well across other image classification challenges to see how well they would perform, however this wasn't allowed as part of the Kaggle competition rules.\n",
    "\n",
    "I really enjoyed working on this project, it has been fantastic to choose a problem that I care deeply about and apply the learning from across this Machine Learning Nanodegree. The most difficult aspect was definitely creating the right CNN architecture to perform well for this problem. My initial CNN choice didn't learn and performed poorly but eventually after several attempts I was able to develop a model that worked well for this challenge.\n",
    "\n",
    "\n",
    "### Improvement\n",
    "\n",
    "If there is one area I would like to improve it would be implementing further data augmentation to identify and isolate the whale flukes within the images. Given the flukes are the sole focus for this challenge the background and additional noise have likely had significant impact on my prediction results. \n",
    "\n",
    "From the initial exploratory analysis it was clear that some of the training images had additional text within the image which would have a large distorting affect on the model.\n",
    "\n",
    "I researched several methods for how to identify whale flukes using similar methods to facial detection. These algorithms were beyond my current skill level to implement within the timeframe of this project but is something I am keen to continue exploring beyond this submission.\n",
    "\n",
    "There is still room for improvement, as seen from the Kaggle Leaderboard and the top score of 0.78563. However, I am pleased with the progress made under this project achieving a score which would rank at 55th out of over 520 global entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* https://www.kaggle.com/c/whale-categorization-playground\n",
    "* http://www.nhm.ac.uk/discover/news/2017/july/museum-unveils-hope-the-blue-whale-skeleton.html\n",
    "* https://happywhale.com/home\n",
    "* https://www.nationalgeographic.com/adventure/adventure-blog/2016/05/04/whos-that-whale-your-photo-could-help-i-d-a-humpback/\n",
    "* https://link.springer.com/chapter/10.1007/3-540-45103-X_16\n",
    "* https://arxiv.org/pdf/1604.05605.pdf\n",
    "* http://www.alaskahumpbacks.org/matching.html\n",
    "* https://en.wikipedia.org/wiki/Perceptual_hashing\n",
    "* https://stackoverflow.com/questions/23660929/how-to-check-whether-a-jpeg-image-is-color-or-gray-scale-using-only-python-stdli\n",
    "* https://github.com/bjlittle/imagehash/blob/master/imagehash/__init__.py\n",
    "* https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/\n",
    "* https://www.manning.com/books/deep-learning-with-python\n",
    "* https://www.kaggle.com/c/whale-categorization-playground/leaderboard\n",
    "* http://cs231n.github.io/convolutional-networks/\n",
    "* https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\n",
    "* https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
